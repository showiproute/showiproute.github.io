<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="悟已往之不谏,知来者之可追">
<meta property="og:type" content="website">
<meta property="og:title" content="积累技术之路">
<meta property="og:url" content="http://blogoflyt.cn/page/4/index.html">
<meta property="og:site_name" content="积累技术之路">
<meta property="og:description" content="悟已往之不谏,知来者之可追">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="积累技术之路">
<meta name="twitter:description" content="悟已往之不谏,知来者之可追">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://blogoflyt.cn/page/4/"/>





  <title>积累技术之路</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">积累技术之路</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">找到学习的热爱</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blogoflyt.cn/2019/03/03/Hadoop学习笔记-集群搭建/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Richard">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/gakki.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="积累技术之路">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/03/Hadoop学习笔记-集群搭建/" itemprop="url">Hadoop学习笔记------集群搭建</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-03T22:01:00+08:00">
                2019-03-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hadoop/" itemprop="url" rel="index">
                    <span itemprop="name">Hadoop</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="hadoop核心组件："><a href="#hadoop核心组件：" class="headerlink" title="hadoop核心组件："></a>hadoop核心组件：</h3><ol>
<li>分布式文件系统：HDFS —— 实现将文件分布式存储在很多的服务器上</li>
<li>分布式运算编程框架：MAPREDUCE —— 实现在很多机器上分布式并行运算</li>
<li>分布式资源调度平台：YARN —— 帮用户调度大量的mapreduce程序，并合理分配运算资源</li>
</ol>
<hr>
<h3 id="hdfs整体运行机制"><a href="#hdfs整体运行机制" class="headerlink" title="hdfs整体运行机制"></a>hdfs整体运行机制</h3><h6 id="hdfs：分布式文件系统"><a href="#hdfs：分布式文件系统" class="headerlink" title="hdfs：分布式文件系统"></a>hdfs：分布式文件系统</h6><p>hdfs有着文件系统共同的特征：</p>
<ol>
<li>有目录结构，顶层目录是：  /</li>
<li>系统中存放的就是文件</li>
<li>系统可以提供对文件的：创建、删除、修改、查看、移动等功能</li>
</ol>
<p>hdfs跟普通的单机文件系统有区别：</p>
<ol>
<li>单机文件系统中存放的文件，是在一台机器的操作系统中</li>
<li>hdfs的文件系统会横跨N多的机器</li>
<li>单机文件系统中存放的文件，是在一台机器的磁盘上</li>
<li>hdfs文件系统中存放的文件，是落在n多机器的本地单机文件系统中（hdfs是一个基于linux本地文件系统之上的文件系统）</li>
</ol>
<h6 id="hdfs的工作机制："><a href="#hdfs的工作机制：" class="headerlink" title="hdfs的工作机制："></a>hdfs的工作机制：</h6><ol>
<li>客户把一个文件存入hdfs，其实hdfs会把这个文件切块后，分散存储在N台linux机器系统中（负责存储文件块的角色：data node）&lt;准确来说：切块的行为是由客户端决定的&gt;</li>
<li>一旦文件被切块存储，那么，hdfs中就必须有一个机制，来记录用户的每一个文件的切块信息，及每一块的具体存储机器（负责记录块信息的角色是：name node）</li>
<li>为了保证数据的安全性，hdfs可以将每一个文件块在集群中存放多个副本（到底存几个副本，是由当时存入该文件的客户端指定的）</li>
</ol>
<blockquote>
<p>综述：一个hdfs系统，由一台运行了namenode的服务器，和N台运行了datanode的服务器组成！</p>
</blockquote>
<hr>
<h4 id="搭建hdfs分布式集群"><a href="#搭建hdfs分布式集群" class="headerlink" title="搭建hdfs分布式集群"></a>搭建hdfs分布式集群</h4><h6 id="hdfs集群组成结构："><a href="#hdfs集群组成结构：" class="headerlink" title="hdfs集群组成结构："></a>hdfs集群组成结构：</h6><p><a href="https://imgchr.com/i/kHADqs" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/02/28/kHADqs.png" alt="kHADqs.png"></a></p>
<h6 id="安装hdfs集群的具体步骤："><a href="#安装hdfs集群的具体步骤：" class="headerlink" title="安装hdfs集群的具体步骤："></a>安装hdfs集群的具体步骤：</h6><ol>
<li><p>首先需要准备N台linux服务器<br>学习阶段，用虚拟机即可！<br>先准备4台虚拟机：1个namenode节点  + 3 个datanode 节点</p>
</li>
<li><p>修改各台机器的主机名和ip地址<br>主机名：hdp-01  对应的ip地址：192.168.33.61<br>主机名：hdp-02  对应的ip地址：192.168.33.62<br>主机名：hdp-03  对应的ip地址：192.168.33.63<br>主机名：hdp-04  对应的ip地址：192.168.33.64</p>
</li>
<li><p>从windows中用CRT软件进行远程连接<br>在windows中将各台linux机器的主机名配置到的windows的本地域名映射文件中：<br>c:/windows/system32/drivers/etc/hosts<br>192.168.33.61    hdp-01<br>192.168.33.62    hdp-02<br>192.168.33.63    hdp-03<br>192.168.33.64    hdp-04</p>
</li>
<li><p>配置linux服务器的基础软件环境</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">防火墙</span><br><span class="line">关闭防火墙：service iptables stop  </span><br><span class="line">关闭防火墙自启： chkconfig iptables off</span><br><span class="line">安装jdk：（hadoop体系中的各软件都是java开发的）</span><br><span class="line">1)利用alt+p 打开sftp窗口，然后将jdk压缩包拖入sftp窗口</span><br><span class="line">2)然后在linux中将jdk压缩包解压到/root/apps 下</span><br><span class="line">3)配置环境变量：JAVA_HOME   PATH</span><br><span class="line">vi /etc/profile   在文件的最后，加入：</span><br><span class="line">export JAVA_HOME=/root/apps/jdk1.8.0_60</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">4)修改完成后，记得 source /etc/profile使配置生效</span><br><span class="line">5)检验：在任意目录下输入命令： java -version 看是否成功执行</span><br><span class="line">6)将安装好的jdk目录用scp命令拷贝到其他机器</span><br><span class="line">7)将/etc/profile配置文件也用scp命令拷贝到其他机器并分别执行source命令</span><br><span class="line">集群内主机的域名映射配置</span><br><span class="line">在hdp-01上，vi /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.33.61   hdp-01</span><br><span class="line">192.168.33.62   hdp-02</span><br><span class="line">192.168.33.63   hdp-03</span><br><span class="line">192.168.33.64   hdp-04</span><br><span class="line">然后，将hosts文件拷贝到集群中的所有其他机器上</span><br><span class="line">scp /etc/hosts hdp-02:/etc/</span><br><span class="line">scp /etc/hosts hdp-03:/etc/</span><br><span class="line">scp /etc/hosts hdp-04:/etc/</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装hdfs集群</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">1、上传hadoop安装包到hdp-01</span><br><span class="line">2、修改配置文件</span><br><span class="line">要点提示	核心配置参数：</span><br><span class="line">1)指定hadoop的默认文件系统为：hdfs</span><br><span class="line">2)指定hdfs的namenode节点为哪台机器</span><br><span class="line">3)指定namenode软件存储元数据的本地目录</span><br><span class="line">4)指定datanode软件存放文件块的本地目录	5)</span><br><span class="line">hadoop的配置文件在：/root/apps/hadoop安装目录/etc/hadoop/</span><br><span class="line">1) 修改hadoop-env.sh</span><br><span class="line">export JAVA_HOME=/root/apps/jdk1.8.0_60</span><br><span class="line">2) 修改core-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hdfs://hdp-01:9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">3) 修改hdfs-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/root/dfs/name&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">&lt;value&gt;/root/dfs/data&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line">4) 拷贝整个hadoop安装目录到其他机器</span><br><span class="line">scp -r /root/apps/hadoop-2.8.0  hdp-02:/root/apps/</span><br><span class="line">scp -r /root/apps/hadoop-2.8.0  hdp-03:/root/apps/</span><br><span class="line">scp -r /root/apps/hadoop-2.8.0  hdp-04:/root/apps/</span><br><span class="line">5) 启动HDFS</span><br><span class="line">所谓的启动HDFS，就是在对的机器上启动对的软件</span><br><span class="line">提示：要运行hadoop的命令，需要在linux环境中配置HADOOP_HOME和PATH环境变量</span><br><span class="line">vi /etc/profile</span><br><span class="line">export JAVA_HOME=/root/apps/jdk1.8.0_60</span><br><span class="line">export HADOOP_HOME=/root/apps/hadoop-2.8.0</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line">首先，初始化namenode的元数据目录</span><br><span class="line">要在hdp-01上执行hadoop的一个命令来初始化namenode的元数据存储目录</span><br><span class="line">hadoop namenode -format</span><br><span class="line">创建一个全新的元数据存储目录</span><br><span class="line">生成记录元数据的文件fsimage</span><br><span class="line">生成集群的相关标识：如：集群id——clusterID</span><br><span class="line">然后，启动namenode进程（在hdp-01上）</span><br><span class="line">hadoop-daemon.sh start namenode</span><br><span class="line">启动完后，首先用jps查看一下namenode的进程是否存在</span><br><span class="line">然后，在windows中用浏览器访问namenode提供的web端口：50070</span><br><span class="line">http://hdp-01:50070</span><br><span class="line">然后，启动众datanode们（在任意地方）</span><br><span class="line">hadoop-daemon.sh start datanode</span><br><span class="line">6) 用自动批量启动脚本来启动HDFS</span><br><span class="line">先配置hdp-01到集群中所有机器（包含自己）的免密登陆</span><br><span class="line">配完免密后，可以执行一次  ssh 0.0.0.0</span><br><span class="line">修改hadoop安装目录中/etc/hadoop/slaves（把需要启动datanode进程的节点列入）</span><br><span class="line">hdp-01</span><br><span class="line">hdp-02</span><br><span class="line">hdp-03</span><br><span class="line">hdp-04</span><br><span class="line">在hdp-01上用脚本：start-dfs.sh 来自动启动整个集群</span><br><span class="line">如果要停止，则用脚本：stop-dfs.sh</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blogoflyt.cn/2019/03/01/Zookeeper介绍/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Richard">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/gakki.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="积累技术之路">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/01/Zookeeper介绍/" itemprop="url">Zookeeper介绍</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-01T16:17:00+08:00">
                2019-03-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ZooKeeper/" itemprop="url" rel="index">
                    <span itemprop="name">ZooKeeper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h6 id="zookeeper-是什么？"><a href="#zookeeper-是什么？" class="headerlink" title="zookeeper 是什么？"></a>zookeeper 是什么？</h6><p>zookeeper 是一个分布式的，开放源码的分布式应用程序协调服务，是 google chubby 的开源实现，是 hadoop 和 hbase 的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。</p>
<h6 id="zookeeper-都有哪些功能？"><a href="#zookeeper-都有哪些功能？" class="headerlink" title="zookeeper 都有哪些功能？"></a>zookeeper 都有哪些功能？</h6><p>集群管理：监控节点存活状态、运行请求等。</p>
<p>主节点选举：主节点挂掉了之后可以从备用的节点开始新一轮选主，主节点选举说的就是这个选举的过程，使用 zookeeper 可以协助完成这个过程。</p>
<p>分布式锁：zookeeper 提供两种锁：独占锁、共享锁。独占锁即一次只能有一个线程使用资源，共享锁是读锁共享，读写互斥，即可以有多线线程同时读同一个资源，如果要使用写锁也只能有一个线程使用。zookeeper可以对分布式锁进行控制。</p>
<p>命名服务：在分布式系统中，通过使用命名服务，客户端应用能够根据指定名字来获取资源或服务的地址，提供者等信息。</p>
<h6 id="zookeeper-有几种部署模式？"><a href="#zookeeper-有几种部署模式？" class="headerlink" title="zookeeper 有几种部署模式？"></a>zookeeper 有几种部署模式？</h6><p>zookeeper 有三种部署模式：</p>
<p>单机部署：一台集群上运行；</p>
<p>集群部署：多台集群运行；</p>
<p>伪集群部署：一台集群启动多个 zookeeper 实例运行。</p>
<h6 id="zookeeper-怎么保证主从节点的状态同步？"><a href="#zookeeper-怎么保证主从节点的状态同步？" class="headerlink" title="zookeeper 怎么保证主从节点的状态同步？"></a>zookeeper 怎么保证主从节点的状态同步？</h6><p>zookeeper 的核心是原子广播，这个机制保证了各个 server 之间的同步。实现这个机制的协议叫做 zab 协议。 zab 协议有两种模式，分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，zab 就进入了恢复模式，当领导者被选举出来，且大多数 server 完成了和 leader 的状态同步以后，恢复模式就结束了。状态同步保证了 leader 和 server 具有相同的系统状态。</p>
<h6 id="集群中为什么要有主节点？"><a href="#集群中为什么要有主节点？" class="headerlink" title="集群中为什么要有主节点？"></a>集群中为什么要有主节点？</h6><p>在分布式环境中，有些业务逻辑只需要集群中的某一台机器进行执行，其他的机器可以共享这个结果，这样可以大大减少重复计算，提高性能，所以就需要主节点。</p>
<h6 id="集群中有-3-台服务器，其中一个节点宕机，这个时候-zookeeper-还可以使用吗？"><a href="#集群中有-3-台服务器，其中一个节点宕机，这个时候-zookeeper-还可以使用吗？" class="headerlink" title="集群中有 3 台服务器，其中一个节点宕机，这个时候 zookeeper 还可以使用吗？"></a>集群中有 3 台服务器，其中一个节点宕机，这个时候 zookeeper 还可以使用吗？</h6><p>可以继续使用，单数服务器只要没超过一半的服务器宕机就可以继续使用。</p>
<h6 id="说一下-zookeeper-的通知机制？"><a href="#说一下-zookeeper-的通知机制？" class="headerlink" title="说一下 zookeeper 的通知机制？"></a>说一下 zookeeper 的通知机制？</h6><p>客户端端会对某个 znode 建立一个 watcher 事件，当该 znode 发生变化时，这些客户端会收到 zookeeper 的通知，然后客户端可以根据 znode 变化来做出业务上的改变。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blogoflyt.cn/2019/02/19/基础知识回顾/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Richard">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/gakki.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="积累技术之路">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/19/基础知识回顾/" itemprop="url">基础知识回顾（网络、WEB、数据库、设计模式）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-19T13:32:00+08:00">
                2019-02-19
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Java/" itemprop="url" rel="index">
                    <span itemprop="name">Java</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h5 id="WEB"><a href="#WEB" class="headerlink" title="WEB"></a>WEB</h5><h6 id="session-和-cookie-有什么区别？"><a href="#session-和-cookie-有什么区别？" class="headerlink" title="session 和 cookie 有什么区别？"></a>session 和 cookie 有什么区别？</h6><p>存储位置不同：session 存储在服务器端；cookie 存储在浏览器端。</p>
<p>安全性不同：cookie 安全性一般，在浏览器存储，可以被伪造和修改。</p>
<p>容量和个数限制：cookie 有容量限制，每个站点下的 cookie 也有个数限制。</p>
<p>存储的多样性：session 可以存储在 Redis 中、数据库中、应用程序中；而 cookie 只能存储在浏览器中。</p>
<h6 id="说一下-session-的工作原理？"><a href="#说一下-session-的工作原理？" class="headerlink" title="说一下 session 的工作原理？"></a>说一下 session 的工作原理？</h6><p>session 的工作原理是客户端登录完成之后，服务器会创建对应的 session，<br>session 创建完之后，会把 session 的 id 发送给客户端，客户端再存储到<br>浏览器中。这样客户端每次访问服务器时，都会带着 sessionid，服务器拿到 sessionid 之后，在内存找到与之对应的 session 这样就可以正常工作了。</p>
<h6 id="如果客户端禁止-cookie-能实现-session-还能用吗？"><a href="#如果客户端禁止-cookie-能实现-session-还能用吗？" class="headerlink" title="如果客户端禁止 cookie 能实现 session 还能用吗？"></a>如果客户端禁止 cookie 能实现 session 还能用吗？</h6><p>可以用，session 只是依赖 cookie 存储 sessionid，如果 cookie 被禁用了，可以使用 url 中添加 sessionid 的方式保证 session 能正常使用。</p>
<h6 id="如何避免-SQL-注入？"><a href="#如何避免-SQL-注入？" class="headerlink" title="如何避免 SQL 注入？"></a>如何避免 SQL 注入？</h6><p>使用预处理 PreparedStatement。</p>
<p>使用正则表达式过滤掉字符中的特殊字符。</p>
<h6 id="什么是-XSS-攻击，如何避免？"><a href="#什么是-XSS-攻击，如何避免？" class="headerlink" title="什么是 XSS 攻击，如何避免？"></a>什么是 XSS 攻击，如何避免？</h6><p>XSS 攻击：即跨站脚本攻击，它是 Web 程序中常见的漏洞。原理是攻击者往 Web 页面里插入恶意的脚本代码（css 代码、Javascript 代码等），当用户浏览该页面时，嵌入其中的脚本代码会被执行，从而达到恶意攻击用户的目的，如盗取用户 cookie、破坏页面结构、重定向到其他网站等。</p>
<p>预防 XSS 的核心是必须对输入的数据做过滤处理。</p>
<h6 id="什么是-CSRF-攻击，如何避免？"><a href="#什么是-CSRF-攻击，如何避免？" class="headerlink" title="什么是 CSRF 攻击，如何避免？"></a>什么是 CSRF 攻击，如何避免？</h6><p>CSRF：Cross-Site Request Forgery（中文：跨站请求伪造），可以理解为攻击者盗用了你的身份，以你的名义发送恶意请求，比如：以你名义发送邮件、发消息、购买商品，虚拟货币转账等。</p>
<p>防御手段：</p>
<p>验证请求来源地址；(HTTP-REFERER)</p>
<p>关键操作添加验证码；</p>
<p>在请求地址添加 token 并验证。（INPUT)</p>
<h5 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h5><h6 id="http-响应码-301-和-302-代表的是什么？有什么区别？"><a href="#http-响应码-301-和-302-代表的是什么？有什么区别？" class="headerlink" title="http 响应码 301 和 302 代表的是什么？有什么区别？"></a>http 响应码 301 和 302 代表的是什么？有什么区别？</h6><p>301：永久重定向。</p>
<p>302：暂时重定向。</p>
<p>它们的区别是，301 对搜索引擎优化（SEO）更加有利；302 有被提示为网络拦截的风险。</p>
<h6 id="forward-和-redirect-的区别？"><a href="#forward-和-redirect-的区别？" class="headerlink" title="forward 和 redirect 的区别？"></a>forward 和 redirect 的区别？</h6><p>forward 是转发 和 redirect 是重定向：</p>
<p>地址栏 url 显示：foward url 不会发生改变，redirect url 会发生改变；<br>数据共享：forward 可以共享 request 里的数据，redirect 不能共享；<br>效率：forward 比 redirect 效率高。</p>
<h6 id="简述-tcp-和-udp的区别？"><a href="#简述-tcp-和-udp的区别？" class="headerlink" title="简述 tcp 和 udp的区别？"></a>简述 tcp 和 udp的区别？</h6><p>tcp 和 udp 是 OSI 模型中的运输层中的协议。tcp 提供可靠的通信传输，而 udp 则常被用于让广播和细节控制交给应用的通信传输。</p>
<p>两者的区别大致如下：</p>
<p>tcp 面向连接，udp 面向非连接即发送数据前不需要建立链接；</p>
<p>tcp 提供可靠的服务（数据传输），udp 无法保证；</p>
<p>tcp 面向字节流，udp 面向报文；</p>
<p>tcp 数据传输慢，udp 数据传输快；</p>
<h6 id="tcp-为什么要三次握手，两次不行吗？为什么？"><a href="#tcp-为什么要三次握手，两次不行吗？为什么？" class="headerlink" title="tcp 为什么要三次握手，两次不行吗？为什么？"></a>tcp 为什么要三次握手，两次不行吗？为什么？</h6><p>如果采用两次握手，那么只要服务器发出确认数据包就会建立连接，但由于客户端此时并未响应服务器端的请求，那此时服务器端就会一直在等待客户端，这样服务器端就白白浪费了一定的资源。若采用三次握手，服务器端没有收到来自客户端的再此确认，则就会知道客户端并没有要求建立请求，就不会浪费服务器的资源。</p>
<h6 id="说一下-tcp-粘包是怎么产生的？"><a href="#说一下-tcp-粘包是怎么产生的？" class="headerlink" title="说一下 tcp 粘包是怎么产生的？"></a>说一下 tcp 粘包是怎么产生的？</h6><p>tcp 粘包可能发生在发送端或者接收端，分别来看两端各种产生粘包的原因：</p>
<p>发送端粘包：发送端需要等缓冲区满才发送出去，造成粘包；</p>
<p>接收方粘包：接收方不及时接收缓冲区的包，造成多个包接收。</p>
<h6 id="OSI-的七层模型都有哪些？"><a href="#OSI-的七层模型都有哪些？" class="headerlink" title="OSI 的七层模型都有哪些？"></a>OSI 的七层模型都有哪些？</h6><p>物理层：利用传输介质为数据链路层提供物理连接，实现比特流的透明传输。</p>
<p>数据链路层：负责建立和管理节点间的链路。</p>
<p>网络层：通过路由选择算法，为报文或分组通过通信子网选择最适当的路径。</p>
<p>传输层：向用户提供可靠的端到端的差错和流量控制，保证报文的正确传输。</p>
<p>会话层：向两个实体的表示层提供建立和使用连接的方法。</p>
<p>表示层：处理用户信息的表示问题，如编码、数据格式转换和加密解密等。</p>
<p>应用层：直接向用户提供服务，完成用户希望在网络上完成的各种工作。</p>
<h6 id="get-和-post-请求有哪些区别？"><a href="#get-和-post-请求有哪些区别？" class="headerlink" title="get 和 post 请求有哪些区别？"></a>get 和 post 请求有哪些区别？</h6><p>get 请求会被浏览器主动缓存，而 post 不会。</p>
<p>get 传递参数有大小限制，而 post 没有。</p>
<p>post 参数传输更安全，get 的参数会明文限制在 url 上，post 不会。</p>
<h5 id="设计模式"><a href="#设计模式" class="headerlink" title="设计模式"></a>设计模式</h5><h6 id="说一下你熟悉的设计模式？"><a href="#说一下你熟悉的设计模式？" class="headerlink" title="说一下你熟悉的设计模式？"></a>说一下你熟悉的设计模式？</h6><p>单例模式：保证被创建一次，节省系统开销。</p>
<p>工厂模式（简单工厂、抽象工厂）：解耦代码。</p>
<p>观察者模式：定义了对象之间的一对多的依赖，这样一来，当一个对象改变时，它的所有的依赖者都会收到通知并自动更新。</p>
<p>外观模式：提供一个统一的接口，用来访问子系统中的一群接口，外观定义了<br>一个高层的接口，让子系统更容易使用。</p>
<p>模版方法模式：定义了一个算法的骨架，而将一些步骤延迟到子类中，模版方法使得子类可以在不改变算法结构的情况下，重新定义算法的步骤。</p>
<p>状态模式：允许对象在内部状态改变时改变它的行为，对象看起来好像修改了它的类。</p>
<h6 id="简单工厂和抽象工厂有什么区别？"><a href="#简单工厂和抽象工厂有什么区别？" class="headerlink" title="简单工厂和抽象工厂有什么区别？"></a>简单工厂和抽象工厂有什么区别？</h6><p>简单工厂：用来生产同一等级结构中的任意产品，对于增加新的产品，无能为力。</p>
<p>工厂方法：用来生产同一等级结构中的固定产品，支持增加任意产品。</p>
<p>抽象工厂：用来生产不同产品族的全部产品，对于增加新的产品，无能为力；支持增加产品族。</p>
<h5 id="MySQL数据库"><a href="#MySQL数据库" class="headerlink" title="MySQL数据库"></a>MySQL数据库</h5><h6 id="数据库的三范式是什么？"><a href="#数据库的三范式是什么？" class="headerlink" title="数据库的三范式是什么？"></a>数据库的三范式是什么？</h6><p>第一范式：强调的是列的原子性，即数据库表的每一列都是不可分割的原子数据项。</p>
<p>第二范式：要求实体的属性完全依赖于主关键字。所谓完全依赖是指不能存在仅依赖主关键字一部分的属性。</p>
<p>第三范式：任何非主属性不依赖于其它非主属性。</p>
<h6 id="一张自增表里面总共有-7-条数据，删除了最后-2-条数据，重启-MySQL-数据库，又插入了一条数据，此时-id-是几？"><a href="#一张自增表里面总共有-7-条数据，删除了最后-2-条数据，重启-MySQL-数据库，又插入了一条数据，此时-id-是几？" class="headerlink" title="一张自增表里面总共有 7 条数据，删除了最后 2 条数据，重启 MySQL 数据库，又插入了一条数据，此时 id 是几？"></a>一张自增表里面总共有 7 条数据，删除了最后 2 条数据，重启 MySQL 数据库，又插入了一条数据，此时 id 是几？</h6><p>表类型如果是 MyISAM ，那 id 就是 8。</p>
<p>表类型如果是 InnoDB，那 id 就是 6。</p>
<p>InnoDB 表只会把自增主键的最大 id 记录在内存中，所以重启之后会导致最大 id 丢失。</p>
<h6 id="说一下-ACID-是什么？"><a href="#说一下-ACID-是什么？" class="headerlink" title="说一下 ACID 是什么？"></a>说一下 ACID 是什么？</h6><p>Atomicity（原子性）：一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被恢复（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。</p>
<p>Consistency（一致性）：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器、级联回滚等。</p>
<p>Isolation（隔离性）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。</p>
<p>Durability（持久性）：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blogoflyt.cn/2019/02/17/sqoop数据迁移工具-导入导出/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Richard">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/gakki.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="积累技术之路">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/17/sqoop数据迁移工具-导入导出/" itemprop="url">sqoop数据迁移工具-----导入导出</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-17T20:23:00+08:00">
                2019-02-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/sqoop/" itemprop="url" rel="index">
                    <span itemprop="name">sqoop</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h5 id="Sqoop的数据导入"><a href="#Sqoop的数据导入" class="headerlink" title="Sqoop的数据导入"></a>Sqoop的数据导入</h5><blockquote>
<p>“导入工具”导入单个表从RDBMS到HDFS。表中的每一行被视为HDFS的记录。所有记录都存储为文本文件的文本数据（或者Avro、sequence文件等二进制数据） </p>
</blockquote>
<h6 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下面的语法用于将数据导入HDFS。</span><br><span class="line">$ sqoop import (generic-args) (import-args)</span><br></pre></td></tr></table></figure>
<h6 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">表数据</span><br><span class="line">在mysql中有一个库userdb中三个表：emp, emp_add和emp_conn</span><br><span class="line">表emp:</span><br><span class="line">id	name	deg	salary	dept</span><br><span class="line">1201	gopal	manager	50,000	TP</span><br><span class="line">1202	manisha	Proof reader	50,000	TP</span><br><span class="line">1203	khalil	php dev	30,000	AC</span><br><span class="line">1204	prasanth	php dev	30,000	AC</span><br><span class="line">1205	kranthi	admin	20,000	TP</span><br><span class="line">表emp_add:</span><br><span class="line">id	hno	street	city</span><br><span class="line">1201	288A	vgiri	jublee</span><br><span class="line">1202	108I	aoc	sec-bad</span><br><span class="line">1203	144Z	pgutta	hyd</span><br><span class="line">1204	78B	old city	sec-bad</span><br><span class="line">1205	720X	hitec	sec-bad</span><br><span class="line">表emp_conn:</span><br><span class="line">id	phno	email</span><br><span class="line">1201	2356742	gopal@tp.com</span><br><span class="line">1202	1661663	manisha@tp.com</span><br><span class="line">1203	8887776	khalil@ac.com</span><br><span class="line">1204	9988774	prasanth@ac.com</span><br><span class="line">1205	1231231	kranthi@tp.com</span><br></pre></td></tr></table></figure>
<h6 id="导入表表数据到HDFS"><a href="#导入表表数据到HDFS" class="headerlink" title="导入表表数据到HDFS"></a>导入表表数据到HDFS</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">下面的命令用于从MySQL数据库服务器中的emp表导入HDFS。</span><br><span class="line">bin/sqoop import   \</span><br><span class="line">--connect jdbc:mysql://hdp-node-01:3306/test   \</span><br><span class="line">--username root  \</span><br><span class="line">--password root   \</span><br><span class="line">--table emp   \</span><br><span class="line">--m 1  </span><br><span class="line">如果成功执行，那么会得到下面的输出。</span><br><span class="line">14/12/22 15:24:54 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5</span><br><span class="line">14/12/22 15:24:56 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/cebe706d23ebb1fd99c1f063ad51ebd7/emp.jar</span><br><span class="line">-----------------------------------------------------</span><br><span class="line">O mapreduce.Job: map 0% reduce 0%</span><br><span class="line">14/12/22 15:28:08 INFO mapreduce.Job: map 100% reduce 0%</span><br><span class="line">14/12/22 15:28:16 INFO mapreduce.Job: Job job_1419242001831_0001 completed successfully</span><br><span class="line">-----------------------------------------------------</span><br><span class="line">-----------------------------------------------------</span><br><span class="line">14/12/22 15:28:17 INFO mapreduce.ImportJobBase: Transferred 145 bytes in 177.5849 seconds (0.8165 bytes/sec)</span><br><span class="line">14/12/22 15:28:17 INFO mapreduce.ImportJobBase: Retrieved 5 records.</span><br></pre></td></tr></table></figure>
<blockquote>
<p>为了验证在HDFS导入的数据，请使用以下命令查看导入的数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ $HADOOP_HOME/bin/hadoop fs -cat /user/hadoop/emp/part-m-00000</span><br><span class="line">emp表的数据和字段之间用逗号(,)表示。</span><br><span class="line">1201, gopal,    manager, 50000, TP</span><br><span class="line">1202, manisha,  preader, 50000, TP</span><br><span class="line">1203, kalil,    php dev, 30000, AC</span><br><span class="line">1204, prasanth, php dev, 30000, AC</span><br><span class="line">1205, kranthi,  admin,   20000, TP</span><br></pre></td></tr></table></figure></p>
</blockquote>
<h6 id="导入到HDFS指定目录"><a href="#导入到HDFS指定目录" class="headerlink" title="导入到HDFS指定目录"></a>导入到HDFS指定目录</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">在导入表数据到HDFS使用Sqoop导入工具，我们可以指定目标目录。</span><br><span class="line">以下是指定目标目录选项的Sqoop导入命令的语法。</span><br><span class="line">--target-dir &lt;new or exist directory in HDFS&gt;</span><br><span class="line">下面的命令是用来导入emp_add表数据到&apos;/queryresult&apos;目录。</span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://hdp-node-01:3306/test \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--target-dir /queryresult \</span><br><span class="line">--fields-terminated-by ‘\001’ \</span><br><span class="line">--table emp </span><br><span class="line">--split-by id</span><br><span class="line">--m 1</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：如果报错，说emp类找不到，则可以手动从sqoop生成的编译目录(/tmp/sqoop-root/compile)中，找到这个emp.class和emp.jar，拷贝到sqoop的lib目录下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果设置了 --m 1，则意味着只会启动一个maptask执行数据导入</span><br><span class="line">如果不设置 --m 1，则默认为启动4个map task执行数据导入，则需要指定一个列来作为划分map task任务的依据</span><br></pre></td></tr></table></figure></p>
</blockquote>
<blockquote>
<p>下面的命令是用来验证 /queryresult 目录中 emp_add表导入的数据形式。<br> $HADOOP_HOME/bin/hadoop fs -cat /queryresult/part-m-*</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">它会用逗号（，）分隔emp_add表的数据和字段。</span><br><span class="line">1201, 288A, vgiri,   jublee</span><br><span class="line">1202, 108I, aoc,     sec-bad</span><br><span class="line">1203, 144Z, pgutta,  hyd</span><br><span class="line">1204, 78B,  oldcity, sec-bad</span><br><span class="line">1205, 720C, hitech,  sec-bad</span><br></pre></td></tr></table></figure>
<h6 id="导入关系表到HIVE"><a href="#导入关系表到HIVE" class="headerlink" title="导入关系表到HIVE"></a>导入关系表到HIVE</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop import --connect jdbc:mysql://hdp-node-01:3306/test --username root --password root --table emp --hive-import  --split-by id  --m 1</span><br></pre></td></tr></table></figure>
<h6 id="导入表数据子集"><a href="#导入表数据子集" class="headerlink" title="导入表数据子集"></a>导入表数据子集</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">我们可以导入表的使用Sqoop导入工具，&quot;where&quot;子句的一个子集。它执行在各自的数据库服务器相应的SQL查询，并将结果存储在HDFS的目标目录。</span><br><span class="line">where子句的语法如下。</span><br><span class="line">--where &lt;condition&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">下面的命令用来导入emp_add表数据的子集。子集查询检索员工ID和地址，居住城市为：Secunderabad</span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://hdp-node-01:3306/test \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--where &quot;city =&apos;sec-bad&apos;&quot; \</span><br><span class="line">--target-dir /wherequery \</span><br><span class="line">--table emp_add \</span><br><span class="line"> --m 1</span><br></pre></td></tr></table></figure>
<h6 id="按需导入"><a href="#按需导入" class="headerlink" title="按需导入"></a>按需导入</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://hdp-node-01:3306/test \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--target-dir /wherequery2 \</span><br><span class="line">--query &apos;select id,name,deg from emp WHERE id&gt;1207 and $CONDITIONS&apos; \</span><br><span class="line">--split-by id \</span><br><span class="line">--fields-terminated-by &apos;\t&apos; \</span><br><span class="line">--m 2</span><br></pre></td></tr></table></figure>
<blockquote>
<p>下面的命令用来验证数据从emp_add表导入/wherequery目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$HADOOP_HOME/bin/hadoop fs -cat /wherequery/part-m-*</span><br><span class="line">它用逗号（，）分隔 emp_add表数据和字段。</span><br><span class="line">1202, 108I, aoc, sec-bad</span><br><span class="line">1204, 78B, oldcity, sec-bad</span><br><span class="line">1205, 720C, hitech, sec-bad</span><br></pre></td></tr></table></figure></p>
</blockquote>
<h6 id="增量导入"><a href="#增量导入" class="headerlink" title="增量导入"></a>增量导入</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">增量导入是仅导入新添加的表中的行的技术。</span><br><span class="line">sqoop支持两种增量MySql导入到hive的模式，</span><br><span class="line"> 	一种是append，即通过指定一个递增的列，比如：</span><br><span class="line">--incremental append  --check-column num_id --last-value 0 </span><br><span class="line">另种是可以根据时间戳，比如：</span><br><span class="line">--incremental lastmodified --check-column created --last-value &apos;2012-02-01 11:0:00&apos; </span><br><span class="line">就是只导入created 比&apos;2012-02-01 11:0:00&apos;更大的数据。</span><br></pre></td></tr></table></figure>
<h6 id="append模式"><a href="#append模式" class="headerlink" title="append模式"></a>append模式</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">它需要添加‘incremental’, ‘check-column’, 和 ‘last-value’选项来执行增量导入。</span><br><span class="line">下面的语法用于Sqoop导入命令增量选项。</span><br><span class="line">--incremental &lt;mode&gt;</span><br><span class="line">--check-column &lt;column name&gt;</span><br><span class="line">--last value &lt;last check column value&gt;</span><br><span class="line">假设新添加的数据转换成emp表如下：</span><br><span class="line">1206, satish p, grp des, 20000, GR</span><br><span class="line">下面的命令用于在EMP表执行增量导入。</span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://hdp-node-01:3306/test \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table emp --m 1 \</span><br><span class="line">--incremental append \</span><br><span class="line">--check-column id \</span><br><span class="line">--last-value 1208</span><br><span class="line">以下命令用于从emp表导入HDFS emp/ 目录的数据验证。</span><br><span class="line">$ $HADOOP_HOME/bin/hadoop fs -cat /user/hadoop/emp/part-m-*</span><br><span class="line">1201, gopal,    manager, 50000, TP</span><br><span class="line">1202, manisha,  preader, 50000, TP</span><br><span class="line">1203, kalil,    php dev, 30000, AC</span><br><span class="line">1204, prasanth, php dev, 30000, AC</span><br><span class="line">1205, kranthi,  admin,   20000, TP</span><br><span class="line">1206, satish p, grp des, 20000, GR</span><br></pre></td></tr></table></figure>
<blockquote>
<p>下面的命令是从表emp 用来查看修改或新添加的行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$HADOOP_HOME/bin/hadoop fs -cat /emp/part-m-*1</span><br><span class="line">1206, satish p, grp des, 20000, GR</span><br><span class="line">2.5 Sqoop的数据导出</span><br></pre></td></tr></table></figure></p>
</blockquote>
<hr>
<h5 id="将数据从HDFS把文件导出到RDBMS数据库"><a href="#将数据从HDFS把文件导出到RDBMS数据库" class="headerlink" title="将数据从HDFS把文件导出到RDBMS数据库"></a>将数据从HDFS把文件导出到RDBMS数据库</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">导出前，目标表必须存在于目标数据库中。</span><br><span class="line">默认操作是从将文件中的数据使用INSERT语句插入到表中</span><br><span class="line">更新模式下，是生成UPDATE语句更新表数据</span><br><span class="line">语法</span><br><span class="line">以下是export命令语法。</span><br><span class="line">$ sqoop export (generic-args) (export-args)</span><br></pre></td></tr></table></figure>
<h6 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">数据是在HDFS 中“EMP/”目录的emp_data文件中。所述emp_data如下：</span><br><span class="line">1201, gopal,     manager, 50000, TP</span><br><span class="line">1202, manisha,   preader, 50000, TP</span><br><span class="line">1203, kalil,     php dev, 30000, AC</span><br><span class="line">1204, prasanth,  php dev, 30000, AC</span><br><span class="line">1205, kranthi,   admin,   20000, TP</span><br><span class="line">1206, satish p,  grp des, 20000, GR</span><br><span class="line"></span><br><span class="line">1、首先需要手动创建mysql中的目标表</span><br><span class="line">$ mysql</span><br><span class="line">mysql&gt; USE db;</span><br><span class="line">mysql&gt; CREATE TABLE employee ( </span><br><span class="line">   id INT NOT NULL PRIMARY KEY, </span><br><span class="line">   name VARCHAR(20), </span><br><span class="line">   deg VARCHAR(20),</span><br><span class="line">   salary INT,</span><br><span class="line">   dept VARCHAR(10));</span><br><span class="line"></span><br><span class="line">2、然后执行导出命令</span><br><span class="line">bin/sqoop export \</span><br><span class="line">--connect jdbc:mysql://hdp-node-01:3306/test \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table employee \</span><br><span class="line">--export-dir /user/hadoop/emp/</span><br><span class="line"></span><br><span class="line">3、验证表mysql命令行。</span><br><span class="line">mysql&gt;select * from employee;</span><br><span class="line">如果给定的数据存储成功，那么可以找到数据在如下的employee表。</span><br><span class="line">+------+--------------+-------------+-------------------+--------+</span><br><span class="line">| Id   | Name         | Designation | Salary            | Dept   |</span><br><span class="line">+------+--------------+-------------+-------------------+--------+</span><br><span class="line">| 1201 | gopal        | manager     | 50000             | TP     |</span><br><span class="line">| 1202 | manisha      | preader     | 50000             | TP     |</span><br><span class="line">| 1203 | kalil        | php dev     | 30000               | AC     |</span><br><span class="line">| 1204 | prasanth     | php dev     | 30000             | AC     |</span><br><span class="line">| 1205 | kranthi      | admin       | 20000             | TP     |</span><br><span class="line">| 1206 | satish p     | grp des     | 20000             | GR     |</span><br><span class="line">+------+--------------+-------------+-------------------+--------+</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blogoflyt.cn/2019/02/16/Java面试题总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Richard">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/gakki.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="积累技术之路">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/16/Java面试题总结/" itemprop="url">Java知识点整理</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-16T22:59:00+08:00">
                2019-02-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Java/" itemprop="url" rel="index">
                    <span itemprop="name">Java</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>转载 <a href="https://gitbook.cn/books/5c6e1937c73f4717175f7477/index.html" target="_blank" rel="noopener">https://gitbook.cn/books/5c6e1937c73f4717175f7477/index.html</a></p>
</blockquote>
<h5 id="Java-基础"><a href="#Java-基础" class="headerlink" title="Java 基础"></a>Java 基础</h5><h6 id="和-equals-的区别是什么"><a href="#和-equals-的区别是什么" class="headerlink" title="== 和 equals 的区别是什么"></a>== 和 equals 的区别是什么</h6><p>== 解读</p>
<p>对于基本类型和引用类型 == 的作用效果是不同的，如下所示：</p>
<p>基本类型：比较的是值是否相同；</p>
<p>引用类型：比较的是引用是否相同；</p>
<p>代码示例：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">String x = &quot;string&quot;;</span><br><span class="line">String y = &quot;string&quot;;</span><br><span class="line">String z = new String(&quot;string&quot;);</span><br><span class="line">System.out.println(x==y); // true</span><br><span class="line">System.out.println(x==z); // false</span><br><span class="line">System.out.println(x.equals(y)); // true</span><br><span class="line">System.out.println(x.equals(z)); // true</span><br><span class="line">代码解读：因为 x 和 y 指向的是同一个引用，所以 == 也是 true，而 new String()方法则重写开辟了内存空间，所以 == 结果为 false，而 equals 比较的一直是值，所以结果都为 true。</span><br></pre></td></tr></table></figure></p>
<p>equals 解读</p>
<p>equals 本质上就是 ==，只不过 String 和 Integer 等重写了 equals 方法，把它变成了值比较。看下面的代码就明白了。</p>
<p>首先来看默认情况下 equals 比较一个有相同值的对象，代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">class Cat &#123;</span><br><span class="line">    public Cat(String name) &#123;</span><br><span class="line">        this.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private String name;</span><br><span class="line"></span><br><span class="line">    public String getName() &#123;</span><br><span class="line">        return name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void setName(String name) &#123;</span><br><span class="line">        this.name = name;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Cat c1 = new Cat(&quot;王磊&quot;);</span><br><span class="line">Cat c2 = new Cat(&quot;王磊&quot;);</span><br><span class="line">System.out.println(c1.equals(c2)); // false</span><br></pre></td></tr></table></figure></p>
<p>输出结果出乎我们的意料，竟然是 false？这是怎么回事，看了 equals 源码就知道了，源码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">public boolean equals(Object obj) &#123;</span><br><span class="line">        return (this == obj);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>原来 equals 本质上就是 ==。</p>
<p>那问题来了，两个相同值的 String 对象，为什么返回的是 true？代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">String s1 = new String(&quot;老王&quot;);</span><br><span class="line">String s2 = new String(&quot;老王&quot;);</span><br><span class="line">System.out.println(s1.equals(s2)); // true</span><br><span class="line">同样的，当我们进入 String 的 equals 方法，找到了答案，代码如下：</span><br><span class="line"></span><br><span class="line">public boolean equals(Object anObject) &#123;</span><br><span class="line">    if (this == anObject) &#123;</span><br><span class="line">        return true;</span><br><span class="line">    &#125;</span><br><span class="line">    if (anObject instanceof String) &#123;</span><br><span class="line">        String anotherString = (String)anObject;</span><br><span class="line">        int n = value.length;</span><br><span class="line">        if (n == anotherString.value.length) &#123;</span><br><span class="line">            char v1[] = value;</span><br><span class="line">            char v2[] = anotherString.value;</span><br><span class="line">            int i = 0;</span><br><span class="line">            while (n-- != 0) &#123;</span><br><span class="line">                if (v1[i] != v2[i])</span><br><span class="line">                    return false;</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">            return true;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return false;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>原来是 String 重写了 Object 的 equals 方法，把引用比较改成了值比较。</p>
<p>总结 ：== 对于基本类型来说是值比较，对于引用类型来说是比较的是引用；而 equals 默认情况下是引用比较，只是很多类重新了 equals 方法，比如 String、Integer 等把它变成了值比较，所以一般情况下 equals 比较的是值是否相等。</p>
<h6 id="Java-中操作字符串都有哪些类？它们之间有什么区别？"><a href="#Java-中操作字符串都有哪些类？它们之间有什么区别？" class="headerlink" title="Java 中操作字符串都有哪些类？它们之间有什么区别？"></a>Java 中操作字符串都有哪些类？它们之间有什么区别？</h6><p>操作字符串的类有：String、StringBuffer、StringBuilder。</p>
<p>String 和 StringBuffer、StringBuilder 的区别在于 String 声明的是不可变的对象，每次操作都会生成新的 String 对象，然后将指针指向新的 String 对象，而 StringBuffer、StringBuilder 可以在原有对象的基础上进行操作，所以在经常改变字符串内容的情况下最好不要使用 String。</p>
<p>StringBuffer 和 StringBuilder 最大的区别在于，StringBuffer 是线程安全的，而 StringBuilder 是非线程安全的，但 StringBuilder 的性能却高于 StringBuffer，所以在单线程环境下推荐使用 StringBuilder，多线程环境下推荐使用 StringBuffer。</p>
<h6 id="抽象类必须要有抽象方法吗？"><a href="#抽象类必须要有抽象方法吗？" class="headerlink" title="抽象类必须要有抽象方法吗？"></a>抽象类必须要有抽象方法吗？</h6><p>不需要，抽象类不一定非要有抽象方法。</p>
<p>示例代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">abstract class Cat &#123;</span><br><span class="line">    public static void sayHi() &#123;</span><br><span class="line">        System. out. println(&quot;hi~&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>上面代码，抽象类并没有抽象方法但完全可以正常运行。</p>
<h6 id="普通类和抽象类有哪些区别？"><a href="#普通类和抽象类有哪些区别？" class="headerlink" title="普通类和抽象类有哪些区别？"></a>普通类和抽象类有哪些区别？</h6><p>普通类不能包含抽象方法，抽象类可以包含抽象方法。<br>抽象类不能直接实例化，普通类可以直接实例化。</p>
<h6 id="抽象类能使用-final-修饰吗？"><a href="#抽象类能使用-final-修饰吗？" class="headerlink" title="抽象类能使用 final 修饰吗？"></a>抽象类能使用 final 修饰吗？</h6><p>不能，定义抽象类就是让其他类继承的，如果定义为 final 该类就不能被继承，这样彼此就会产生矛盾，所以 final 不能修饰抽象类，如下图所示，编辑器也会提示错误信息：</p>
<p><a href="https://imgchr.com/i/AZaSQ1" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/03/16/AZaSQ1.png" alt="AZaSQ1.png"></a></p>
<h6 id="接口和抽象类有什么区别？"><a href="#接口和抽象类有什么区别？" class="headerlink" title="接口和抽象类有什么区别？"></a>接口和抽象类有什么区别？</h6><p>实现：抽象类的子类使用 extends 来继承；接口必须使用 implements 来实现接口。</p>
<p>构造函数：抽象类可以有构造函数；接口不能有。</p>
<p>实现数量：类可以实现很多个接口；但是只能继承一个抽象类。</p>
<p>访问修饰符：接口中的方法默认使用 public 修饰；抽象类中的方法可以是任意访问修饰符。</p>
<h6 id="Java-中-IO-流分为几种？"><a href="#Java-中-IO-流分为几种？" class="headerlink" title="Java 中 IO 流分为几种？"></a>Java 中 IO 流分为几种？</h6><p>按功能来分：输入流（input）、输出流（output）。</p>
<p>按类型来分：字节流和字符流。</p>
<p>字节流和字符流的区别是：字节流按 8 位传输以字节为单位输入输出数据，字符流按 16 位传输以字符为单位输入输出数据。</p>
<h6 id="BIO、NIO、AIO-有什么区别？"><a href="#BIO、NIO、AIO-有什么区别？" class="headerlink" title="BIO、NIO、AIO 有什么区别？"></a>BIO、NIO、AIO 有什么区别？</h6><p>BIO：Block IO 同步阻塞式 IO，就是我们平常使用的传统 IO，它的特点是模式简单使用方便，并发处理能力低。</p>
<p>NIO：New IO 同步非阻塞 IO，是传统 IO 的升级，客户端和服务器端通过<br>Channel（通道）通讯，实现了多路复用。</p>
<p>AIO：Asynchronous IO 是 NIO 的升级，也叫 NIO2，实现了异步非堵塞 IO ，异步 IO 的操作基于事件和回调机制。</p>
<h5 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h5><h6 id="Java-容器都有哪些？"><a href="#Java-容器都有哪些？" class="headerlink" title="Java 容器都有哪些？"></a>Java 容器都有哪些？</h6><p>Java 容器分为 Collection 和 Map 两大类，其下又有很多子类，如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Collection</span><br><span class="line">List</span><br><span class="line">	ArrayList</span><br><span class="line">	LinkedList</span><br><span class="line">	Vector</span><br><span class="line">	Stack</span><br><span class="line">Set</span><br><span class="line">	HashSet</span><br><span class="line">	LinkedHashSet</span><br><span class="line">	TreeSet</span><br><span class="line">Map</span><br><span class="line">	HashMap</span><br><span class="line">	LinkedHashMap</span><br><span class="line">	TreeMap</span><br><span class="line">ConcurrentHashMap</span><br><span class="line">Hashtable</span><br></pre></td></tr></table></figure></p>
<h6 id="List、Set、Map-之间的区别是什么？"><a href="#List、Set、Map-之间的区别是什么？" class="headerlink" title="List、Set、Map 之间的区别是什么？"></a>List、Set、Map 之间的区别是什么？</h6><p>List、Set、Map 的区别主要体现在两个方面：元素是否有序、是否允许元素重复。</p>
<p>三者之间的区别，如下表：</p>
<p><a href="https://imgchr.com/i/AZaAFe" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/03/16/AZaAFe.png" alt="AZaAFe.png"></a></p>
<h6 id="如何决定使用-HashMap-还是-TreeMap？"><a href="#如何决定使用-HashMap-还是-TreeMap？" class="headerlink" title="如何决定使用 HashMap 还是 TreeMap？"></a>如何决定使用 HashMap 还是 TreeMap？</h6><p>对于在 Map 中插入、删除、定位一个元素这类操作，HashMap 是最好的选择，因为相对而言 HashMap 的插入会更快，但如果你要对一个 key 集合进行有序的遍历，那 TreeMap 是更好的选择。</p>
<h6 id="说一下-HashMap-的实现原理？"><a href="#说一下-HashMap-的实现原理？" class="headerlink" title="说一下 HashMap 的实现原理？"></a>说一下 HashMap 的实现原理？</h6><p>HashMap 基于 Hash 算法实现的，我们通过 put(key,value)存储，get(key)来获取。当传入 key 时，HashMap 会根据 key. hashCode() 计算出 hash 值，根据 hash 值将 value 保存在 bucket 里。当计算出的 hash 值相同时，我们称之为 hash 冲突，HashMap 的做法是用链表和红黑树存储相同 hash 值的 value。当 hash 冲突的个数比较少时，使用链表否则使用红黑树。</p>
<h6 id="说一下-HashSet-的实现原理？"><a href="#说一下-HashSet-的实现原理？" class="headerlink" title="说一下 HashSet 的实现原理？"></a>说一下 HashSet 的实现原理？</h6><p>HashSet 是基于 HashMap 实现的，HashSet 底层使用 HashMap 来保存所有元素，因此 HashSet 的实现比较简单，相关 HashSet 的操作，基本上都是直接调用底层 HashMap 的相关方法来完成，HashSet 不允许重复的值。</p>
<h6 id="ArrayList-和-LinkedList-的区别是什么？"><a href="#ArrayList-和-LinkedList-的区别是什么？" class="headerlink" title="ArrayList 和 LinkedList 的区别是什么？"></a>ArrayList 和 LinkedList 的区别是什么？</h6><p>数据结构实现：ArrayList 是动态数组的数据结构实现，而 LinkedList 是双向链表的数据结构实现。</p>
<p>随机访问效率：ArrayList 比 LinkedList 在随机访问的时候效率要高，因为 LinkedList 是线性的数据存储方式，所以需要移动指针从前往后依次查找。</p>
<p>增加和删除效率：在非首尾的增加和删除操作，LinkedList 要比 ArrayList 效率要高，因为 ArrayList 增删操作要影响数组内的其他数据的下标。</p>
<p>综合来说，在需要频繁读取集合中的元素时，更推荐使用 ArrayList，而在插入和删除操作较多时，更推荐使用 LinkedList。</p>
<h6 id="如何实现数组和-List-之间的转换？"><a href="#如何实现数组和-List-之间的转换？" class="headerlink" title="如何实现数组和 List 之间的转换？"></a>如何实现数组和 List 之间的转换？</h6><p>数组转 List：使用 Arrays. asList(array) 进行转换。</p>
<p>List 转数组：使用 List 自带的 toArray() 方法。</p>
<p>代码示例：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// list to array</span><br><span class="line">List&lt;String&gt; list = new ArrayList&lt;String&gt;();</span><br><span class="line">list. add(&quot;王磊&quot;);</span><br><span class="line">list. add(&quot;的博客&quot;);</span><br><span class="line">list. toArray();</span><br><span class="line">// array to list</span><br><span class="line">String[] array = new String[]&#123;&quot;王磊&quot;,&quot;的博客&quot;&#125;;</span><br><span class="line">Arrays. asList(array);</span><br></pre></td></tr></table></figure></p>
<h6 id="ArrayList-和-Vector-的区别是什么？"><a href="#ArrayList-和-Vector-的区别是什么？" class="headerlink" title="ArrayList 和 Vector 的区别是什么？"></a>ArrayList 和 Vector 的区别是什么？</h6><p>线程安全：Vector 使用了 Synchronized 来实现线程同步，是线程安全的，而 ArrayList 是非线程安全的。</p>
<p>性能：ArrayList 在性能方面要优于 Vector。</p>
<p>扩容：ArrayList 和 Vector 都会根据实际的需要动态的调整容量，只不过在 Vector 扩容每次会增加 1 倍，而 ArrayList 只会增加 50%。</p>
<h5 id="Array-和-ArrayList-有何区别？"><a href="#Array-和-ArrayList-有何区别？" class="headerlink" title="Array 和 ArrayList 有何区别？"></a>Array 和 ArrayList 有何区别？</h5><p>Array 可以存储基本数据类型和对象，ArrayList 只能存储对象。</p>
<p>Array 是指定固定大小的，而 ArrayList 大小是自动扩展的。</p>
<p>Array 内置方法没有 ArrayList 多，比如 addAll、removeAll、iteration 等方法只有 ArrayList 有。</p>
<h6 id="在-Queue-中-poll-和-remove-有什么区别？"><a href="#在-Queue-中-poll-和-remove-有什么区别？" class="headerlink" title="在 Queue 中 poll()和 remove()有什么区别？"></a>在 Queue 中 poll()和 remove()有什么区别？</h6><p>相同点：都是返回第一个元素，并在队列中删除返回的对象。</p>
<p>不同点：如果没有元素 poll()会返回 null，而 remove()会直接抛出 NoSuchElementException 异常。</p>
<p>代码示例：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Queue&lt;String&gt; queue = new LinkedList&lt;String&gt;();</span><br><span class="line">queue. offer(&quot;string&quot;); // add</span><br><span class="line">System. out. println(queue. poll());</span><br><span class="line">System. out. println(queue. remove());</span><br><span class="line">System. out. println(queue. size());</span><br></pre></td></tr></table></figure></p>
<h5 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h5><h6 id="并行和并发有什么区别？"><a href="#并行和并发有什么区别？" class="headerlink" title="并行和并发有什么区别？"></a>并行和并发有什么区别？</h6><p>并行：多个处理器或多核处理器同时处理多个任务。</p>
<p>并发：多个任务在同一个 CPU 核上，按细分的时间片轮流(交替)执行，从逻辑上来看那些任务是同时执行。</p>
<h6 id="线程和进程的区别"><a href="#线程和进程的区别" class="headerlink" title="线程和进程的区别"></a>线程和进程的区别</h6><p>一个程序下至少有一个进程，一个进程下至少有一个线程，一个进程下也可以有多个线程来增加程序的执行速度。</p>
<h6 id="守护线程是什么"><a href="#守护线程是什么" class="headerlink" title="守护线程是什么"></a>守护线程是什么</h6><p>守护线程是运行在后台的一种特殊进程。它独立于控制终端并且周期性地执行某种任务或等待处理某些发生的事件。在 Java 中垃圾回收线程就是特殊的守护线程。</p>
<h6 id="创建线程有哪几种方式？"><a href="#创建线程有哪几种方式？" class="headerlink" title="创建线程有哪几种方式？"></a>创建线程有哪几种方式？</h6><p>创建线程有三种方式：</p>
<p>继承 Thread 重新 run 方法；</p>
<p>实现 Runnable 接口；</p>
<p>实现 Callable 接口。</p>
<h6 id="说一下-runnable-和-callable-有什么区别？"><a href="#说一下-runnable-和-callable-有什么区别？" class="headerlink" title="说一下 runnable 和 callable 有什么区别？"></a>说一下 runnable 和 callable 有什么区别？</h6><p>runnable 没有返回值，callable 可以拿到有返回值，callable 可以看作是 runnable 的补充。</p>
<h6 id="sleep-和-wait-有什么区别？"><a href="#sleep-和-wait-有什么区别？" class="headerlink" title="sleep() 和 wait() 有什么区别？"></a>sleep() 和 wait() 有什么区别？</h6><p>类的不同：sleep() 来自 Thread，wait() 来自 Object。</p>
<p>释放锁：sleep() 不释放锁；wait() 释放锁。</p>
<p>用法不同：sleep() 时间到会自动恢复；wait() 可以使用 </p>
<p>notify()/notifyAll()直接唤醒。</p>
<h6 id="线程的-run-和-start-有什么区别？"><a href="#线程的-run-和-start-有什么区别？" class="headerlink" title="线程的 run() 和 start() 有什么区别？"></a>线程的 run() 和 start() 有什么区别？</h6><p>start() 方法用于启动线程，run() 方法用于执行线程的运行时代码。run() 可以重复调用，而 start() 只能调用一次。</p>
<h6 id="创建线程池有哪几种方式？"><a href="#创建线程池有哪几种方式？" class="headerlink" title="创建线程池有哪几种方式？"></a>创建线程池有哪几种方式？</h6><p>线程池创建有七种方式，最核心的是最后一种：</p>
<p>newSingleThreadExecutor()：它的特点在于工作线程数目被限制为 1，操作一个无界的工作队列，所以它保证了所有任务的都是被顺序执行，最多会有一个任务处于活动状态，并且不允许使用者改动线程池实例，因此可以避免其改变线程数目；</p>
<p>newCachedThreadPool()：它是一种用来处理大量短时间工作任务的线程池，具有几个鲜明特点：它会试图缓存线程并重用，当无缓存线程可用时，就会创建新的工作线程；如果线程闲置的时间超过 60 秒，则被终止并移出缓存；长时间闲置时，这种线程池，不会消耗什么资源。其内部使用 SynchronousQueue 作为工作队列；</p>
<p>newFixedThreadPool(int nThreads)：重用指定数目（nThreads）的线程，其背后使用的是无界的工作队列，任何时候最多有 nThreads 个工作线程是活动的。这意味着，如果任务数量超过了活动队列数目，将在工作队列中等待空闲线程出现；如果有工作线程退出，将会有新的工作线程被创建，以补足指定的数目 nThreads；</p>
<p>newSingleThreadScheduledExecutor()：创建单线程池，返回 ScheduledExecutorService，可以进行定时或周期性的工作调度；</p>
<p>newScheduledThreadPool(int corePoolSize)：和newSingleThreadScheduledExecutor()类似，创建的是个 ScheduledExecutorService，可以进行定时或周期性的工作调度，区别在于单一工作线程还是多个工作线程；</p>
<p>newWorkStealingPool(int parallelism)：这是一个经常被人忽略的线程池，Java 8 才加入这个创建方法，其内部会构建ForkJoinPool，利用Work-Stealing算法，并行地处理任务，不保证处理顺序；</p>
<p>ThreadPoolExecutor()：是最原始的线程池创建，上面1-3创建方式都是对ThreadPoolExecutor的封装。</p>
<h6 id="在-Java-程序中怎么保证多线程的运行安全？"><a href="#在-Java-程序中怎么保证多线程的运行安全？" class="headerlink" title="在 Java 程序中怎么保证多线程的运行安全？"></a>在 Java 程序中怎么保证多线程的运行安全？</h6><p>方法一：使用安全类，比如 Java. util. concurrent 下的类。</p>
<p>方法二：使用自动锁 synchronized。</p>
<p>方法三：使用手动锁 Lock。</p>
<p>手动锁 Java 示例代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Lock lock = new ReentrantLock();</span><br><span class="line">lock. lock();</span><br><span class="line">try &#123;</span><br><span class="line">    System. out. println(&quot;获得锁&quot;);</span><br><span class="line">&#125; catch (Exception e) &#123;</span><br><span class="line">    // TODO: handle exception</span><br><span class="line">&#125; finally &#123;</span><br><span class="line">    System. out. println(&quot;释放锁&quot;);</span><br><span class="line">    lock. unlock();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h6 id="什么是死锁？"><a href="#什么是死锁？" class="headerlink" title="什么是死锁？"></a>什么是死锁？</h6><p>当线程 A 持有独占锁a，并尝试去获取独占锁 b 的同时，线程 B 持有独占锁 b，并尝试获取独占锁 a 的情况下，就会发生 AB 两个线程由于互相持有对方需要的锁，而发生的阻塞现象，我们称为死锁。</p>
<h6 id="怎么防止死锁？"><a href="#怎么防止死锁？" class="headerlink" title="怎么防止死锁？"></a>怎么防止死锁？</h6><p>尽量使用 tryLock(long timeout, TimeUnit unit)的方法(ReentrantLock、ReentrantReadWriteLock)，设置超时时间，超时可以退出防止死锁。</p>
<p>尽量使用 Java. util. concurrent 并发类代替自己手写锁。</p>
<p>尽量降低锁的使用粒度，尽量不要几个功能用同一把锁。</p>
<p>尽量减少同步的代码块。</p>
<h6 id="synchronized-和-volatile-的区别是什么？"><a href="#synchronized-和-volatile-的区别是什么？" class="headerlink" title="synchronized 和 volatile 的区别是什么？"></a>synchronized 和 volatile 的区别是什么？</h6><p>volatile 是变量修饰符；synchronized 是修饰类、方法、代码段。</p>
<p>volatile 仅能实现变量的修改可见性，不能保证原子性；而 synchronized<br>则可以保证变量的修改可见性和原子性。</p>
<p>volatile 不会造成线程的阻塞；synchronized 可能会造成线程的阻塞。</p>
<h6 id="synchronized-和-Lock-有什么区别？"><a href="#synchronized-和-Lock-有什么区别？" class="headerlink" title="synchronized 和 Lock 有什么区别？"></a>synchronized 和 Lock 有什么区别？</h6><p>synchronized 可以给类、方法、代码块加锁；而 lock 只能给代码块加锁。</p>
<p>synchronized 不需要手动获取锁和释放锁，使用简单，发生异常会自动释放锁，不会造成死锁；而 lock 需要自己加锁和释放锁，如果使用不当没有 </p>
<p>unLock()去释放锁就会造成死锁。<br>通过 Lock 可以知道有没有成功获取锁，而 synchronized 却无法办到。</p>
<h5 id="反射"><a href="#反射" class="headerlink" title="反射"></a>反射</h5><h6 id="什么是反射？"><a href="#什么是反射？" class="headerlink" title="什么是反射？"></a>什么是反射？</h6><p>反射是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为 Java 语言的反射机制。</p>
<h5 id="对象拷贝"><a href="#对象拷贝" class="headerlink" title="对象拷贝"></a>对象拷贝</h5><h6 id="为什么要使用克隆？"><a href="#为什么要使用克隆？" class="headerlink" title="为什么要使用克隆？"></a>为什么要使用克隆？</h6><p>克隆的对象可能包含一些已经修改过的属性，而 new 出来的对象的属性都还是初始化时候的值，所以当需要一个新的对象来保存当前对象的“状态”就靠克隆方法了。</p>
<h6 id="如何实现对象克隆？"><a href="#如何实现对象克隆？" class="headerlink" title="如何实现对象克隆？"></a>如何实现对象克隆？</h6><p>实现 Cloneable 接口并重写 Object 类中的 clone() 方法。<br>实现 Serializable 接口，通过对象的序列化和反序列化实现克隆，可以实现真正的深度克隆。</p>
<h6 id="深拷贝和浅拷贝区别是什么？"><a href="#深拷贝和浅拷贝区别是什么？" class="headerlink" title="深拷贝和浅拷贝区别是什么？"></a>深拷贝和浅拷贝区别是什么？</h6><p>浅克隆：当对象被复制时只复制它本身和其中包含的值类型的成员变量，而引用类型的成员对象并没有复制。<br>深克隆：除了对象本身被复制外，对象所包含的所有成员变量也将复制。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blogoflyt.cn/2019/02/16/sqoop数据迁移工具-安装配置/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Richard">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/gakki.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="积累技术之路">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/16/sqoop数据迁移工具-安装配置/" itemprop="url">sqoop数据迁移工具-----安装配置</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-16T20:20:00+08:00">
                2019-02-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/sqoop/" itemprop="url" rel="index">
                    <span itemprop="name">sqoop</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>–安装配置<br>author: Richard<br>tags:</p>
<ul>
<li>sqoop</li>
<li>数据迁移<br>categories:</li>
<li>sqoop<br>date: 2019-02-16 20:20:00</li>
</ul>
<hr>
<h5 id="sqoop数据迁移工具"><a href="#sqoop数据迁移工具" class="headerlink" title="sqoop数据迁移工具"></a>sqoop数据迁移工具</h5><h6 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sqoop是apache旗下一款“Hadoop和关系数据库服务器之间传送数据”的工具。</span><br><span class="line">导入数据：MySQL，Oracle导入数据到Hadoop的HDFS、HIVE、HBASE等数据存储系统；</span><br><span class="line">导出数据：从Hadoop的文件系统中导出数据到关系数据库mysql等</span><br></pre></td></tr></table></figure>
<p><a href="https://imgchr.com/i/kzviTA" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/03/08/kzviTA.jpg" alt="kzviTA.jpg"></a></p>
<h6 id="工作机制"><a href="#工作机制" class="headerlink" title="工作机制"></a>工作机制</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">将导入或导出命令翻译成mapreduce程序来实现</span><br><span class="line">在翻译出的mapreduce中主要是对inputformat和outputformat进行定制</span><br></pre></td></tr></table></figure>
<h5 id="sqoop安装"><a href="#sqoop安装" class="headerlink" title="sqoop安装"></a>sqoop安装</h5><blockquote>
<p>安装sqoop的前提是已经具备java和hadoop的环境</p>
<ol>
<li>下载并解压<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">最新版下载地址http://ftp.wayne.edu/apache/sqoop/1.4.6/</span><br></pre></td></tr></table></figure>
</li>
</ol>
</blockquote>
<ol start="2">
<li><p>修改配置文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ cd $SQOOP_HOME/conf</span><br><span class="line">$ mv sqoop-env-template.sh sqoop-env.sh</span><br><span class="line">打开sqoop-env.sh并编辑下面几行：</span><br><span class="line">export HADOOP_COMMON_HOME=/home/hadoop/apps/hadoop-2.6.1/ </span><br><span class="line">export HADOOP_MAPRED_HOME=/home/hadoop/apps/hadoop-2.6.1/</span><br><span class="line">export HIVE_HOME=/home/hadoop/apps/hive-1.2.1</span><br></pre></td></tr></table></figure>
</li>
<li><p>加入mysql的jdbc驱动包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp  ~/app/hive/lib/mysql-connector-java-5.1.28.jar   $SQOOP_HOME/lib/</span><br></pre></td></tr></table></figure>
</li>
<li><p>验证启动</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ cd $SQOOP_HOME/bin</span><br><span class="line">$ sqoop-version</span><br><span class="line">预期的输出：</span><br><span class="line">15/12/17 14:52:32 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6</span><br><span class="line">Sqoop 1.4.6 git commit id 5b34accaca7de251fc91161733f906af2eddbe83</span><br><span class="line">Compiled by abe on Fri Aug 1 11:19:26 PDT 2015</span><br><span class="line">到这里，整个Sqoop安装工作完成。</span><br><span class="line"></span><br><span class="line">验证sqoop到mysql业务库之间的连通性：</span><br><span class="line">bin/sqoop-list-databases --connect jdbc:mysql://localhost:3306 --username root --password root</span><br><span class="line">bin/sqoop-list-tables --connect jdbc:mysql://localhost:3306/userdb --username root --password root</span><br></pre></td></tr></table></figure></li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blogoflyt.cn/2019/02/14/Flume日志采集框架-高可用/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Richard">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/gakki.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="积累技术之路">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/14/Flume日志采集框架-高可用/" itemprop="url">Flume日志采集框架-----高可用</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-14T19:40:00+08:00">
                2019-02-14
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Flume/" itemprop="url" rel="index">
                    <span itemprop="name">Flume</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h5 id="FLUME高级应用"><a href="#FLUME高级应用" class="headerlink" title="FLUME高级应用"></a>FLUME高级应用</h5><h6 id="案例：高可用Flum-NG配置案例"><a href="#案例：高可用Flum-NG配置案例" class="headerlink" title="案例：高可用Flum-NG配置案例"></a>案例：高可用Flum-NG配置案例</h6><blockquote>
<p>在完成单点的Flume NG搭建后，下面我们搭建一个高可用的Flume NG集群，架构图如下所示：<br><a href="https://imgchr.com/i/kzOPwn" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/03/08/kzOPwn.png" alt="kzOPwn.png"></a></p>
</blockquote>
<blockquote>
<p>图中，我们可以看出，Flume的存储可以支持多种，这里只列举了HDFS和Kafka（如：存储最新的一周日志，并给Storm系统提供实时日志流）。</p>
</blockquote>
<h6 id="角色分配"><a href="#角色分配" class="headerlink" title="角色分配"></a>角色分配</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Flume的Agent和Collector分布如下表所示：</span><br><span class="line">名称　	HOST	角色</span><br><span class="line">Agent1	mini1	Web Server</span><br><span class="line">Agent2	mini2	Web Server</span><br><span class="line">Agent3	mini3	Web Server</span><br><span class="line">Collector1	mini4	AgentMstr1</span><br><span class="line">Collector2	mini5	AgentMstr2</span><br></pre></td></tr></table></figure>
<blockquote>
<p>图中所示，Agent1，Agent2，Agent3数据分别流入到Collector1和Collector2，Flume NG本身提供了Failover机制，可以自动切换和恢复。在上图中，有3个产生日志服务器分布在不同的机房，要把所有的日志都收集到一个集群中存储。下 面我们开发配置Flume NG集群</p>
</blockquote>
<h6 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">在下面单点Flume中，基本配置都完成了，我们只需要新添加两个配置文件，它们是agent.properties和collector.properties，其配置内容如下所示：</span><br><span class="line">1、agent配置</span><br><span class="line">[root@mini1 apache-flume-1.6.0-bin]# vi conf/agent.properties</span><br><span class="line">#agent1 name</span><br><span class="line">agent1.channels = c1</span><br><span class="line">agent1.sources = r1</span><br><span class="line">agent1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line">#set gruop</span><br><span class="line">agent1.sinkgroups = g1</span><br><span class="line"></span><br><span class="line">#set channel</span><br><span class="line">agent1.channels.c1.type = memory</span><br><span class="line">agent1.channels.c1.capacity = 1000</span><br><span class="line">agent1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">agent1.sources.r1.channels = c1</span><br><span class="line">agent1.sources.r1.type = exec</span><br><span class="line">agent1.sources.r1.command = tail -F /root/log/test.log</span><br><span class="line"></span><br><span class="line">agent1.sources.r1.interceptors = i1 i2</span><br><span class="line">agent1.sources.r1.interceptors.i1.type = static</span><br><span class="line">agent1.sources.r1.interceptors.i1.key = Type</span><br><span class="line">agent1.sources.r1.interceptors.i1.value = LOGIN</span><br><span class="line">agent1.sources.r1.interceptors.i2.type = timestamp</span><br><span class="line"></span><br><span class="line"># set sink1</span><br><span class="line">agent1.sinks.k1.channel = c1</span><br><span class="line">agent1.sinks.k1.type = avro</span><br><span class="line">agent1.sinks.k1.hostname = mini2</span><br><span class="line">agent1.sinks.k1.port = 52020</span><br><span class="line"></span><br><span class="line"># set sink2</span><br><span class="line">agent1.sinks.k2.channel = c1</span><br><span class="line">agent1.sinks.k2.type = avro</span><br><span class="line">agent1.sinks.k2.hostname = mini3</span><br><span class="line">agent1.sinks.k2.port = 52020</span><br><span class="line"></span><br><span class="line">#set sink group</span><br><span class="line">agent1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line"></span><br><span class="line">#set failover</span><br><span class="line">agent1.sinkgroups.g1.processor.type = failover</span><br><span class="line">agent1.sinkgroups.g1.processor.priority.k1 = 10</span><br><span class="line">agent1.sinkgroups.g1.processor.priority.k2 = 1</span><br><span class="line">agent1.sinkgroups.g1.processor.maxpenalty = 10000</span><br><span class="line">启动命令：</span><br><span class="line">bin/flume-ng agent -n agent1 -c conf -f conf/agent.properties -Dflume.root.logger=DEBUG,console</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2、collector配置</span><br><span class="line">[root@mini2 conf]# vi collector.properties </span><br><span class="line">#set Agent name</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line">a1.sinks = k1</span><br><span class="line"></span><br><span class="line">#set channel</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># other node,nna to nns</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = mini2</span><br><span class="line">a1.sources.r1.port = 52020</span><br><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = static</span><br><span class="line">a1.sources.r1.interceptors.i1.key = Collector</span><br><span class="line">a1.sources.r1.interceptors.i1.value = mini2</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line"></span><br><span class="line">#set sink to hdfs</span><br><span class="line">a1.sinks.k1.type=hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path=/home/hdfs/flume/logdfs</span><br><span class="line">a1.sinks.k1.hdfs.fileType=DataStream</span><br><span class="line">a1.sinks.k1.hdfs.writeFormat=TEXT</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval=10</span><br><span class="line">a1.sinks.k1.channel=c1</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix=%Y-%m-%d</span><br><span class="line">在mini3上，需要修改上述配置中的红色字体主机名为mini3</span><br></pre></td></tr></table></figure>
<h6 id="启动命令："><a href="#启动命令：" class="headerlink" title="启动命令："></a>启动命令：</h6><p>bin/flume-ng agent -n a1 -c conf -f conf/collector.properties -Dflume.root.logger=DEBUG,console</p>
<h6 id="FAILOVER测试"><a href="#FAILOVER测试" class="headerlink" title="FAILOVER测试"></a>FAILOVER测试</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下面我们来测试下Flume NG集群的高可用（故障转移）。场景如下：我们在Agent1节点上传文件，由于我们配置Collector1的权重比Collector2大，所以 Collector1优先采集并上传到存储系统。然后我们kill掉Collector1，此时有Collector2负责日志的采集上传工作，之后，我 们手动恢复Collector1节点的Flume服务，再次在Agent1上次文件，发现Collector1恢复优先级别的采集工作。具体截图如下所 示：</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Collector1优先上传<br><a href="https://imgchr.com/i/kzOelF" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/03/08/kzOelF.png" alt="kzOelF.png"></a><br>HDFS集群中上传的log内容预览<br><a href="https://imgchr.com/i/kzOmy4" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/03/08/kzOmy4.png" alt="kzOmy4.png"></a><br>Collector1宕机，Collector2获取优先上传权限<br><a href="https://imgchr.com/i/kzOnOJ" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/03/08/kzOnOJ.png" alt="kzOnOJ.png"></a><br>重启Collector1服务，Collector1重新获得优先上传的权限</p>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blogoflyt.cn/2019/02/13/Flume日志采集框架-采集案例/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Richard">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/gakki.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="积累技术之路">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/13/Flume日志采集框架-采集案例/" itemprop="url">Flume日志采集框架-----采集案例</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T19:33:00+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Flume/" itemprop="url" rel="index">
                    <span itemprop="name">Flume</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h5 id="采集案例"><a href="#采集案例" class="headerlink" title="采集案例"></a>采集案例</h5><h6 id="1、采集日志目录中的文件到HDFS"><a href="#1、采集日志目录中的文件到HDFS" class="headerlink" title="1、采集日志目录中的文件到HDFS"></a>1、采集日志目录中的文件到HDFS</h6><blockquote>
<p>结构示意图：</p>
</blockquote>
<p><a href="https://imgchr.com/i/kzLssJ" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/03/08/kzLssJ.png" alt="kzLssJ.png"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">采集需求：某服务器的某特定目录下，会不断产生新的文件，每当有新文件出现，就需要把文件采集到HDFS中去</span><br><span class="line">根据需求，首先定义以下3大要素</span><br><span class="line">数据源组件，即source ——监控文件目录 :  spooldir</span><br><span class="line">spooldir特性：</span><br><span class="line">   1、监视一个目录，只要目录中出现新文件，就会采集文件中的内容</span><br><span class="line">   2、采集完成的文件，会被agent自动添加一个后缀：COMPLETED</span><br><span class="line">   3、所监视的目录中不允许重复出现相同文件名的文件</span><br><span class="line">下沉组件，即sink——HDFS文件系统  :  hdfs sink</span><br><span class="line">通道组件，即channel——可用file channel 也可以用内存channel</span><br><span class="line"></span><br><span class="line">配置文件编写：</span><br><span class="line">#定义三大组件的名称</span><br><span class="line">agent1.sources = source1</span><br><span class="line">agent1.sinks = sink1</span><br><span class="line">agent1.channels = channel1</span><br><span class="line"></span><br><span class="line"># 配置source组件</span><br><span class="line">agent1.sources.source1.type = spooldir</span><br><span class="line">agent1.sources.source1.spoolDir = /home/hadoop/logs/</span><br><span class="line">agent1.sources.source1.fileHeader = false</span><br><span class="line"></span><br><span class="line">#配置拦截器</span><br><span class="line">agent1.sources.source1.interceptors = i1</span><br><span class="line">agent1.sources.source1.interceptors.i1.type = host</span><br><span class="line">agent1.sources.source1.interceptors.i1.hostHeader = hostname</span><br><span class="line"></span><br><span class="line"># 配置sink组件</span><br><span class="line">agent1.sinks.sink1.type = hdfs</span><br><span class="line">agent1.sinks.sink1.hdfs.path =hdfs://hdp-node-01:9000/weblog/flume-collection/%y-%m-%d/%H-%M</span><br><span class="line">agent1.sinks.sink1.hdfs.filePrefix = access_log</span><br><span class="line">agent1.sinks.sink1.hdfs.maxOpenFiles = 5000</span><br><span class="line">agent1.sinks.sink1.hdfs.batchSize= 100</span><br><span class="line">agent1.sinks.sink1.hdfs.fileType = DataStream</span><br><span class="line">agent1.sinks.sink1.hdfs.writeFormat =Text</span><br><span class="line">agent1.sinks.sink1.hdfs.rollSize = 102400</span><br><span class="line">agent1.sinks.sink1.hdfs.rollCount = 1000000</span><br><span class="line">agent1.sinks.sink1.hdfs.rollInterval = 60</span><br><span class="line">#agent1.sinks.sink1.hdfs.round = true</span><br><span class="line">#agent1.sinks.sink1.hdfs.roundValue = 10</span><br><span class="line">#agent1.sinks.sink1.hdfs.roundUnit = minute</span><br><span class="line">agent1.sinks.sink1.hdfs.useLocalTimeStamp = true</span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">agent1.channels.channel1.type = memory</span><br><span class="line">agent1.channels.channel1.keep-alive = 120</span><br><span class="line">agent1.channels.channel1.capacity = 500000</span><br><span class="line">agent1.channels.channel1.transactionCapacity = 600</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">agent1.sources.source1.channels = channel1</span><br><span class="line">agent1.sinks.sink1.channel = channel1</span><br><span class="line"></span><br><span class="line">Channel参数解释：</span><br><span class="line">capacity：默认该通道中最大的可以存储的event数量</span><br><span class="line">trasactionCapacity：每次最大可以从source中拿到或者送到sink中的event数量</span><br><span class="line">keep-alive：event添加到通道中或者移出的允许时间</span><br><span class="line"></span><br><span class="line">测试阶段，启动flume agent的命令：</span><br><span class="line">bin/flume-ng  agent  -c  ./conf  -f ./dir-hdfs.conf -n  agent1 -Dflume.root.logger=DEBUG,console</span><br><span class="line">-D后面跟的是log4j的参数，用于测试观察</span><br><span class="line"></span><br><span class="line">生产中，启动flume，应该把flume启动在后台：</span><br><span class="line">nohup bin/flume-ng  agent  -c  ./conf  -f ./dir-hdfs.conf -n  agent1 1&gt;/dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure></p>
<h6 id="采集文件到HDFS"><a href="#采集文件到HDFS" class="headerlink" title="采集文件到HDFS"></a>采集文件到HDFS</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">采集需求：比如业务系统使用log4j生成的日志，日志内容不断增加，需要把追加到日志文件中的数据实时采集到hdfs</span><br><span class="line"></span><br><span class="line">根据需求，首先定义以下3大要素</span><br><span class="line">采集源，即source——监控文件内容更新 :  exec  ‘tail -F file’</span><br><span class="line">下沉目标，即sink——HDFS文件系统  :  hdfs sink</span><br><span class="line">Source和sink之间的传递通道——channel，可用file channel 也可以用 内存channel</span><br><span class="line"></span><br><span class="line">配置文件编写：</span><br><span class="line">agent1.sources = source1</span><br><span class="line">agent1.sinks = sink1</span><br><span class="line">agent1.channels = channel1</span><br><span class="line"></span><br><span class="line"># Describe/configure tail -F source1</span><br><span class="line">agent1.sources.source1.type = exec</span><br><span class="line">agent1.sources.source1.command = tail -F /home/hadoop/logs/access_log</span><br><span class="line">agent1.sources.source1.channels = channel1</span><br><span class="line"></span><br><span class="line">#configure host for source</span><br><span class="line">agent1.sources.source1.interceptors = i1</span><br><span class="line">agent1.sources.source1.interceptors.i1.type = host</span><br><span class="line">agent1.sources.source1.interceptors.i1.hostHeader = hostname</span><br><span class="line"></span><br><span class="line"># Describe sink1</span><br><span class="line">agent1.sinks.sink1.type = hdfs</span><br><span class="line">#a1.sinks.k1.channel = c1</span><br><span class="line">agent1.sinks.sink1.hdfs.path =hdfs://hdp-node-01:9000/weblog/flume-collection/%y-%m-%d/%H-%M</span><br><span class="line">agent1.sinks.sink1.hdfs.filePrefix = access_log</span><br><span class="line">agent1.sinks.sink1.hdfs.maxOpenFiles = 5000</span><br><span class="line">agent1.sinks.sink1.hdfs.batchSize= 100</span><br><span class="line">agent1.sinks.sink1.hdfs.fileType = DataStream</span><br><span class="line">agent1.sinks.sink1.hdfs.writeFormat =Text</span><br><span class="line">agent1.sinks.sink1.hdfs.rollSize = 102400</span><br><span class="line">agent1.sinks.sink1.hdfs.rollCount = 1000000</span><br><span class="line">agent1.sinks.sink1.hdfs.rollInterval = 60</span><br><span class="line">agent1.sinks.sink1.hdfs.round = true</span><br><span class="line">agent1.sinks.sink1.hdfs.roundValue = 10</span><br><span class="line">agent1.sinks.sink1.hdfs.roundUnit = minute</span><br><span class="line">agent1.sinks.sink1.hdfs.useLocalTimeStamp = true</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">agent1.channels.channel1.type = memory</span><br><span class="line">agent1.channels.channel1.keep-alive = 120</span><br><span class="line">agent1.channels.channel1.capacity = 500000</span><br><span class="line">agent1.channels.channel1.transactionCapacity = 600</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">agent1.sources.source1.channels = channel1</span><br><span class="line">agent1.sinks.sink1.channel = channel1</span><br></pre></td></tr></table></figure>
<h6 id="两个agent级联"><a href="#两个agent级联" class="headerlink" title="两个agent级联"></a>两个agent级联</h6><p><a href="https://imgchr.com/i/kzLTLd" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/03/08/kzLTLd.png" alt="kzLTLd.png"></a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">从tail命令获取数据发送到avro端口</span><br><span class="line">另一个节点可配置一个avro源来中继数据，发送外部存储</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##################</span><br><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">a1.sources.r1.command = tail -F /root/log/access.log</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = avro</span><br><span class="line">a1.sinks.k1.hostname = hdp-05</span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line">a1.sinks.k1.batch-size = 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">从avro端口接收数据，下沉到hdfs</span><br><span class="line">#####</span><br><span class="line">bin/flume-ng agent -c conf -f conf/avro-m-log.conf -n a1 -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">采集配置文件，avro-hdfs.conf</span><br><span class="line"></span><br><span class="line"># Name the components on this agent</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source</span><br><span class="line">##source中的avro组件是一个接收者服务</span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = hdp-05</span><br><span class="line">a1.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Describe the sink</span><br><span class="line">a1.sinks.k1.type = hdfs</span><br><span class="line">a1.sinks.k1.hdfs.path = /flume/taildata/%y-%m-%d/</span><br><span class="line">a1.sinks.k1.hdfs.filePrefix = tail-</span><br><span class="line">a1.sinks.k1.hdfs.round = true</span><br><span class="line">a1.sinks.k1.hdfs.roundValue = 24</span><br><span class="line">a1.sinks.k1.hdfs.roundUnit = hour</span><br><span class="line">a1.sinks.k1.hdfs.rollInterval = 0</span><br><span class="line">a1.sinks.k1.hdfs.rollSize = 0</span><br><span class="line">a1.sinks.k1.hdfs.rollCount = 50</span><br><span class="line">a1.sinks.k1.hdfs.batchSize = 10</span><br><span class="line">a1.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本</span><br><span class="line">a1.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">发送数据：</span><br><span class="line">$ bin/flume-ng avro-client -H localhost -p 4141 -F /usr/logs/log.10</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blogoflyt.cn/2019/02/13/Flume日志采集框架-安装配置/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Richard">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/gakki.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="积累技术之路">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/13/Flume日志采集框架-安装配置/" itemprop="url">Flume日志采集框架-----安装配置</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T19:23:00+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Flume/" itemprop="url" rel="index">
                    <span itemprop="name">Flume</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h5 id="Flume介绍"><a href="#Flume介绍" class="headerlink" title="Flume介绍"></a>Flume介绍</h5><h6 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Flume是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。</span><br><span class="line">Flume可以采集文件，socket数据包、文件、文件夹、kafka等各种形式源数据，又可以将采集到的数据(下沉sink)输出到HDFS、hbase、hive、kafka等众多外部存储系统中</span><br><span class="line">一般的采集需求，通过对flume的简单配置即可实现</span><br><span class="line">Flume针对特殊场景也具备良好的自定义扩展能力，</span><br><span class="line">因此，flume可以适用于大部分的日常数据采集场景</span><br></pre></td></tr></table></figure>
<h6 id="运行机制"><a href="#运行机制" class="headerlink" title="运行机制"></a>运行机制</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1、Flume分布式系统中最核心的角色是agent，flume采集系统就是由一个个agent所连接起来形成</span><br><span class="line">2、每一个agent相当于一个数据传递员[Source 到 Channel 到 Sink之间传递数据的形式是Event事件；Event事件是一个数据流单元。</span><br><span class="line">]，内部有三个组件：</span><br><span class="line">a)Source：采集组件，用于跟数据源对接，以获取数据</span><br><span class="line">b)Sink：下沉组件，用于往下一级agent传递数据或者往最终存储系统传递数据</span><br><span class="line">c)Channel：传输通道组件，用于从source将数据传递到sink</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Source 到 Channel 到 Sink之间传递数据的形式是Event事件；Event事件是一个数据流单元。</p>
</blockquote>
<p><a href="https://imgchr.com/i/kzLVrd" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/03/08/kzLVrd.png" alt="kzLVrd.png"></a></p>
<h5 id="Flume采集系统结构图"><a href="#Flume采集系统结构图" class="headerlink" title="Flume采集系统结构图"></a>Flume采集系统结构图</h5><h6 id="1-简单结构"><a href="#1-简单结构" class="headerlink" title="1. 简单结构"></a>1. 简单结构</h6><blockquote>
<p>单个agent采集数据<br><a href="https://imgchr.com/i/kzLnat" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/03/08/kzLnat.png" alt="kzLnat.png"></a></p>
</blockquote>
<h6 id="复杂结构"><a href="#复杂结构" class="headerlink" title="复杂结构"></a>复杂结构</h6><blockquote>
<p>多级agent之间串联<br><a href="https://imgchr.com/i/kzLMPf" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/03/08/kzLMPf.png" alt="kzLMPf.png"></a></p>
</blockquote>
<h5 id="Flume实战案例"><a href="#Flume实战案例" class="headerlink" title="Flume实战案例"></a>Flume实战案例</h5><h6 id="1-2-1-Flume的安装部署"><a href="#1-2-1-Flume的安装部署" class="headerlink" title="1.2.1 Flume的安装部署"></a>1.2.1 Flume的安装部署</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1、Flume的安装非常简单，只需要解压即可，当然，前提是已有hadoop环境</span><br><span class="line">上传安装包到数据源所在节点上</span><br><span class="line">然后解压  tar -zxvf apache-flume-1.6.0-bin.tar.gz</span><br><span class="line">然后进入flume的目录，修改conf下的flume-env.sh，在里面配置JAVA_HOME</span><br><span class="line"></span><br><span class="line">2、根据数据采集的需求配置采集方案，描述在配置文件中(文件名可任意自定义)</span><br><span class="line">3、指定采集方案配置文件，在相应的节点上启动flume agent</span><br><span class="line"></span><br><span class="line">先用一个最简单的例子来测试一下程序环境是否正常</span><br></pre></td></tr></table></figure>
<p><a href="https://imgchr.com/i/kzLlRS" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/03/08/kzLlRS.png" alt="kzLlRS.png"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">1、先在flume的conf目录下新建一个配置文件（采集方案）</span><br><span class="line">vi   netcat-logger.properties</span><br><span class="line"># 定义这个agent中各组件的名字</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># 描述和配置source组件：r1</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># 描述和配置sink组件：k1</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># 描述和配置channel组件，此处使用是内存缓存的方式</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># 描述和配置source  channel   sink之间的连接关系</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2、启动agent去采集数据</span><br><span class="line">bin/flume-ng agent -c conf -f conf/netcat-logger.conf -n a1  -Dflume.root.logger=INFO,console</span><br><span class="line">-c conf   指定flume自身的配置文件所在目录</span><br><span class="line">-f conf/netcat-logger.con  指定我们所描述的采集方案</span><br><span class="line">-n a1  指定我们这个agent的名字</span><br><span class="line"></span><br><span class="line">3、测试</span><br><span class="line">先要往agent的source所监听的端口上发送数据，让agent有数据可采</span><br><span class="line">随便在一个能跟agent节点联网的机器上</span><br><span class="line">telnet anget-hostname  port   （telnet localhost 44444）</span><br></pre></td></tr></table></figure></p>
<p><a href="https://imgchr.com/i/kzLNaq" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/03/08/kzLNaq.png" alt="kzLNaq.png"></a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blogoflyt.cn/2019/02/10/HBASE运行原理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Richard">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/gakki.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="积累技术之路">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/10/HBASE运行原理/" itemprop="url">HBASE运行原理</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-10T21:04:00+08:00">
                2019-02-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/HBASE/" itemprop="url" rel="index">
                    <span itemprop="name">HBASE</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h5 id="组件结构图"><a href="#组件结构图" class="headerlink" title="组件结构图"></a>组件结构图</h5><p><a href="https://imgchr.com/i/kxR4LF" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/03/07/kxR4LF.png" alt="kxR4LF.png"></a></p>
<h6 id="MASTER职责"><a href="#MASTER职责" class="headerlink" title="MASTER职责"></a>MASTER职责</h6><ol>
<li>管理HRegionServer，实现其负载均衡。<br>管理和分配HRegion，比如在HRegion split时分配新的HRegion；在HRegionServer退出时迁移其负责的HRegion到其他HRegionServer上。</li>
<li>Admin职能<br>创建、删除、修改Table的定义。实现DDL操作（namespace和table的增删改，column familiy的增删改等）。<br>管理namespace和table的元数据（实际存储在HDFS上）。<br>权限控制（ACL）。<br>监控集群中所有HRegionServer的状态(通过Heartbeat和监听ZooKeeper中的状态)。</li>
</ol>
<h6 id="REGION-SERVER职责"><a href="#REGION-SERVER职责" class="headerlink" title="REGION SERVER职责"></a>REGION SERVER职责</h6><ol>
<li>管理自己所负责的region数据的读写。</li>
<li>读写HDFS，管理Table中的数据。</li>
<li>Client直接通过HRegionServer读写数据（从HMaster中获取元数据，找到RowKey所在的HRegion/HRegionServer后）。</li>
</ol>
<h6 id="Zookeeper集群所起作用"><a href="#Zookeeper集群所起作用" class="headerlink" title="Zookeeper集群所起作用"></a>Zookeeper集群所起作用</h6><ol>
<li>存放整个HBase集群的元数据以及集群的状态信息。</li>
<li>实现HMaster主从节点的failover。<blockquote>
<p>注： HMaster通过监听ZooKeeper中的Ephemeral节点(默认：/hbase/rs/*)来监控HRegionServer的加入和宕机。<br>在第一个HMaster连接到ZooKeeper时会创建Ephemeral节点(默认：/hbasae/master)来表示Active的HMaster，其后加进来的HMaster则监听该Ephemeral节点<br>如果当前Active的HMaster宕机，则该节点消失，因而其他HMaster得到通知，而将自身转换成Active的HMaster，在变为Active的HMaster之前，它会在/hbase/masters/下创建自己的Ephemeral节点。</p>
</blockquote>
</li>
</ol>
<h5 id="HBASE读写数据流程"><a href="#HBASE读写数据流程" class="headerlink" title="HBASE读写数据流程"></a>HBASE读写数据流程</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1. 在HBase 0.96以前，HBase有两个特殊的Table：-ROOT-和.META. 用来记录用户表的rowkey范围所在的的regionserver服务器；因而客户端读写数据时需要通过3次寻址请求来对数据所在的regionserver进行定位，效率低下；</span><br></pre></td></tr></table></figure>
<p><a href="https://imgchr.com/i/kxRTo9" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/03/07/kxRTo9.jpg" alt="kxRTo9.jpg"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2. 而在HBase 0.96以后去掉了-ROOT- Table，只剩下这个特殊的目录表叫做Meta Table(hbase:meta)，它存储了集群中所有用户HRegion的位置信息，而ZooKeeper的节点中(/hbase/meta-region-server)存储的则直接是这个Meta Table的位置，并且这个Meta Table如以前的-ROOT- Table一样是不可split的。这样，客户端在第一次访问用户Table的流程就变成了：</span><br><span class="line">①　从ZooKeeper(/hbase/meta-region-server)中获取hbase:meta的位置（HRegionServer的位置），缓存该位置信息。</span><br><span class="line">②　从HRegionServer中查询用户Table对应请求的RowKey所在的HRegionServer，缓存该位置信息。</span><br><span class="line">③　从查询到HRegionServer中读取Row。</span><br><span class="line">注：客户会缓存这些位置信息，然而第二步它只是缓存当前RowKey对应的HRegion的位置，因而如果下一个要查的RowKey不在同一个HRegion中，则需要继续查询hbase:meta所在的HRegion，然而随着时间的推移，客户端缓存的位置信息越来越多，以至于不需要再次查找hbase:meta Table的信息，除非某个HRegion因为宕机或Split被移动，此时需要重新查询并且更新缓存。</span><br></pre></td></tr></table></figure></p>
<h6 id="hbase-meta表"><a href="#hbase-meta表" class="headerlink" title="hbase:meta表"></a>hbase:meta表</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hbase:meta表存储了所有用户HRegion的位置信息：</span><br><span class="line">Rowkey：tableName,regionStartKey,regionId,replicaId等；</span><br><span class="line">info列族：这个列族包含三个列，他们分别是：</span><br><span class="line">info:regioninfo列：</span><br><span class="line">regionId,tableName,startKey,endKey,offline,split,replicaId；</span><br><span class="line">info:server列：HRegionServer对应的server:port；</span><br><span class="line">info:serverstartcode列：HRegionServer的启动时间戳。</span><br></pre></td></tr></table></figure>
<p><a href="https://imgchr.com/i/kxRLz6" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/03/07/kxRLz6.png" alt="kxRLz6.png"></a></p>
<h6 id="REGION-SERVER内部机制"><a href="#REGION-SERVER内部机制" class="headerlink" title="REGION SERVER内部机制"></a>REGION SERVER内部机制</h6><p><a href="https://imgchr.com/i/kxRvLD" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/03/07/kxRvLD.png" alt="kxRvLD.png"></a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">WAL即Write Ahead Log，在早期版本中称为HLog，它是HDFS上的一个文件，如其名字所表示的，所有写操作都会先保证将数据写入这个Log文件后，才会真正更新MemStore，最后写入HFile中。WAL文件存储在/hbase/WALs/$&#123;HRegionServer_Name&#125;的目录中</span><br><span class="line"></span><br><span class="line">BlockCache是一个读缓存，即“引用局部性”原理（也应用于CPU，分空间局部性和时间局部性，空间局部性是指CPU在某一时刻需要某个数据，那么有很大的概率在一下时刻它需要的数据在其附近；时间局部性是指某个数据在被访问过一次后，它有很大的概率在不久的将来会被再次的访问），将数据预读取到内存中，以提升读的性能。</span><br><span class="line"></span><br><span class="line">HRegion是一个Table中的一个Region在一个HRegionServer中的表达。一个Table可以有一个或多个Region，他们可以在一个相同的HRegionServer上，也可以分布在不同的HRegionServer上，一个HRegionServer可以有多个HRegion，他们分别属于不同的Table。HRegion由多个Store(HStore)构成，每个HStore对应了一个Table在这个HRegion中的一个Column Family，即每个Column Family就是一个集中的存储单元，因而最好将具有相近IO特性的Column存储在一个Column Family，以实现高效读取(数据局部性原理，可以提高缓存的命中率)。HStore是HBase中存储的核心，它实现了读写HDFS功能，一个HStore由一个MemStore 和0个或多个StoreFile组成。</span><br><span class="line"></span><br><span class="line">MemStore是一个写缓存(In Memory Sorted Buffer)，所有数据的写在完成WAL日志写后，会 写入MemStore中，由MemStore根据一定的算法将数据Flush到地层HDFS文件中(HFile)，通常每个HRegion中的每个 Column Family有一个自己的MemStore。</span><br><span class="line"></span><br><span class="line">HFile(StoreFile) 用于存储HBase的数据(Cell/KeyValue)。在HFile中的数据是按RowKey、Column Family、Column排序，对相同的Cell(即这三个值都一样)，则按timestamp倒序排列。</span><br><span class="line"></span><br><span class="line">FLUSH详述</span><br><span class="line"></span><br><span class="line">①　每一次Put/Delete请求都是先写入到MemStore中，当MemStore满后会Flush成一个新的StoreFile(底层实现是HFile)，即一个HStore(Column Family)可以有0个或多个StoreFile(HFile)。</span><br><span class="line"></span><br><span class="line">②　当一个HRegion中的所有MemStore的大小总和超过了hbase.hregion.memstore.flush.size的大小，默认128MB。此时当前的HRegion中所有的MemStore会Flush到HDFS中。</span><br><span class="line"></span><br><span class="line">③　当全局MemStore的大小超过了hbase.regionserver.global.memstore.upperLimit的大小，默认40％的内存使用量。此时当前HRegionServer中所有HRegion中的MemStore都会Flush到HDFS中，Flush顺序是MemStore大小的倒序（一个HRegion中所有MemStore总和作为该HRegion的MemStore的大小还是选取最大的MemStore作为参考？有待考证），直到总体的MemStore使用量低于hbase.regionserver.global.memstore.lowerLimit，默认38%的内存使用量。</span><br><span class="line"></span><br><span class="line">④　当前HRegionServer中WAL的大小超过了</span><br><span class="line">hbase.regionserver.hlog.blocksize * hbase.regionserver.max.logs</span><br><span class="line">的数量，当前HRegionServer中所有HRegion中的MemStore都会Flush到HDFS中，</span><br><span class="line">Flush使用时间顺序，最早的MemStore先Flush直到WAL的数量少于</span><br><span class="line">hbase.regionserver.hlog.blocksize * hbase.regionserver.max.logs</span><br><span class="line">这里说这两个相乘的默认大小是2GB，查代码，hbase.regionserver.max.logs默认值是32，而hbase.regionserver.hlog.blocksize默认是32MB。但不管怎么样，因为这个大小超过限制引起的Flush不是一件好事，可能引起长时间的延迟</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/gakki.jpg"
                alt="Richard" />
            
              <p class="site-author-name" itemprop="name">Richard</p>
              <p class="site-description motion-element" itemprop="description">悟已往之不谏,知来者之可追</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">72</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">92</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://github.com/showiproute" title="Github" target="_blank">Github</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Richard</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
